# ğŸŒ³ Decision Tree Classifier â€” Pima Indians Diabetes Dataset

## ğŸ§­ Objective
The aim of this practical is to **classify individuals as diabetic or non-diabetic** using the **Decision Tree algorithm** on the **Pima Indians Diabetes dataset**.  
The experiment explores how different health-related factors such as **Glucose**, **BMI**, and **Age** contribute to diabetes prediction.  
Both **Entropy (Information Gain)** and **Gini Index** criteria are used to evaluate model performance and determine the best root node.

---

## ğŸ“Š Dataset Description
The dataset named **`diabetes.csv`** contains diagnostic measurements of female Pima Indian patients aged 21 years or older.  
It is used to predict whether a patient has diabetes based on certain physiological features.

| Column | Description |
|---------|-------------|
| **Pregnancies** | Number of times the patient was pregnant |
| **Glucose** | Plasma glucose concentration after 2 hours |
| **BloodPressure** | Diastolic blood pressure (mm Hg) |
| **SkinThickness** | Triceps skin fold thickness (mm) |
| **Insulin** | 2-hour serum insulin (mu U/ml) |
| **BMI** | Body Mass Index (weight in kg / height in mÂ²) |
| **DiabetesPedigreeFunction** | Diabetes heredity function score |
| **Age** | Age of the individual (years) |
| **Outcome** | Target variable â€” 0 (Non-diabetic), 1 (Diabetic) |

---

## ğŸ§° Tools & Libraries Used

| Tool/Library | Purpose |
|---------------|----------|
| **Python (v3.x)** | Programming environment |
| **pandas** | Data manipulation and preprocessing |
| **NumPy** | Mathematical and statistical computations |
| **scikit-learn** | Model building and evaluation (Decision Tree, train-test split) |
| **Matplotlib** | Visualization of decision tree and data insights |
| **Jupyter Notebook / Google Colab** | Code execution and result visualization |

---

## âš™ï¸ Methodology

### 1ï¸âƒ£ Data Loading and Exploration
- Load the dataset using **pandas** and examine its structure, missing values, and summary statistics.

### 2ï¸âƒ£ Feature Selection
- Independent variables: All columns except `Outcome`  
- Dependent variable: `Outcome`

### 3ï¸âƒ£ Train-Test Split
-Split data for training and testing.

### 4ï¸âƒ£ Model Building
- Build two models using DecisionTreeClassifier:

```python
  dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
  dt_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
```
### 5ï¸âƒ£ Visualization
- Visualized both the decision trees.

### 6ï¸âƒ£ Manual Calculation
-Compute Entropy, Information Gain, and Gini Index for the feature Glucose to verify root node selection.

## ğŸ§¾ Results Summary

| Criterion      | Root Node | Accuracy | Remarks                  |
| -------------- | --------- | -------- | ------------------------ |
| **Entropy**    | Glucose   | ~75%     | Highest Information Gain |
| **Gini Index** | Glucose   | ~75%     | Highest Gini Reduction   |

-Both criteria produced similar accuracy and tree structures.
-Glucose was identified as the root node due to maximum impurity reduction.
-BMI and Age appeared as secondary splitting factors.

## ğŸ§® Key Formulas Used
## ğŸ“Š Entropy, Gini Index, and Information Gain

## ğŸ“Š Entropy, Gini Index, and Information Gain

### ğŸ”¹ Entropy
Entropy measures the impurity or uncertainty in a dataset.  
It is calculated using the formula:

**Entropy = âˆ’ Î£ ( páµ¢ Ã— logâ‚‚(páµ¢) )**

Where:  
- *páµ¢* = probability of class *i*

---

### ğŸ”¹ Gini Index
The Gini Index measures the impurity in a dataset and is calculated as:

**Gini = 1 âˆ’ Î£ ( páµ¢Â² )**

Where:  
- *páµ¢* = probability of class *i*

---

### ğŸ”¹ Information Gain
Information Gain (IG) represents the reduction in entropy after splitting the dataset based on an attribute.  
It is calculated as:

**IG = Entropy(parent) âˆ’ Î£ ( (n_child / n_total) Ã— Entropy(child) )**

Where:  
- *Entropy(parent)* = Entropy of the original dataset  
- *n_child* = number of samples in each child node  
- *n_total* = total number of samples before the split  
- *Entropy(child)* = Entropy after the split
